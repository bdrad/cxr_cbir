{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import  nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm.notebook import tqdm\nimport cv2\nfrom torchvision import transforms, utils\nimport skimage.io as skio\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-27T08:23:15.976901Z","iopub.execute_input":"2021-12-27T08:23:15.977174Z","iopub.status.idle":"2021-12-27T08:23:15.984773Z","shell.execute_reply.started":"2021-12-27T08:23:15.977137Z","shell.execute_reply":"2021-12-27T08:23:15.983671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_metadata = pd.read_csv(\"/kaggle/input/data/Data_Entry_2017.csv\")\n_metadata.loc[_metadata[\"Finding Labels\"].str.contains(\"No Finding\")]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:15.986565Z","iopub.execute_input":"2021-12-27T08:23:15.987919Z","iopub.status.idle":"2021-12-27T08:23:16.310308Z","shell.execute_reply.started":"2021-12-27T08:23:15.987829Z","shell.execute_reply":"2021-12-27T08:23:16.309531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:16.311878Z","iopub.execute_input":"2021-12-27T08:23:16.312637Z","iopub.status.idle":"2021-12-27T08:23:23.813180Z","shell.execute_reply.started":"2021-12-27T08:23:16.312596Z","shell.execute_reply":"2021-12-27T08:23:23.812122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import einops\ndef show_image(image):\n    plt.figure()\n    plt.imshow(image)\n    plt.show()\n\ndef rearrange_tensor(img):\n    return einops.rearrange(img, \"c w h -> w h c\")\n\ndef unnormalize_tensor(img):   \n    img = img.detach().numpy()\n#     img = einops.rearrange(img, \"b c w h -> w h (b c)\")\n    img = einops.rearrange(img, \"c w h -> w h c\")\n\n    mean = np.array([0.485, 0.456, 0.406])\n    std =  np.array([0.229, 0.224, 0.225])\n    img = (img * std) + mean\n    return img\n\ndef unnormalize_img(img):   \n    mean = np.array([0.485, 0.456, 0.406])\n    std =  np.array([0.229, 0.224, 0.225])\n    img = (img * std) + mean\n    return img\n\n\ndef normalize_image(img):\n    # normalize all images, this is necessary prepreocessing of inputs for vgg network\n    normalize = transforms.Compose([\n                    transforms.ToPILImage(),\n                    transforms.Resize(256), \n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.485, 0.456, 0.406), \n                                          (0.229, 0.224, 0.225))\n                    ])\n    img = normalize(img)\n#     img = img[:3,:,:].unsqueeze(0)\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:23.816734Z","iopub.execute_input":"2021-12-27T08:23:23.817135Z","iopub.status.idle":"2021-12-27T08:23:23.829232Z","shell.execute_reply.started":"2021-12-27T08:23:23.817086Z","shell.execute_reply":"2021-12-27T08:23:23.827410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChestDataset(Dataset):\n    def __init__(self, class_name = \"Pneumonia\", val = False):\n        _metadata = pd.read_csv(\"/kaggle/input/data/Data_Entry_2017.csv\")\n        _metadata = _metadata.sample(frac = 1)\n        metadata_true = _metadata.loc[_metadata[\"Finding Labels\"].str.contains(class_name)]\n        metadata_false = _metadata.loc[_metadata[\"Finding Labels\"].str.contains(\"No Finding\")]\n        num_train = int(.6 * len(metadata_true))\n        num_val = int(.4 * len(metadata_true))\n        train_metadata = pd.concat([metadata_true[:num_train], metadata_false[:num_train]])\n        val_metadata = pd.concat([metadata_true[num_train:], metadata_false[num_train : num_train + num_val]])\n        self.metadata = pd.concat([train_metadata, val_metadata])\n        self.num_train = num_train\n        self.num_val = num_val\n        self.class_name = class_name\n        self.data = {}\n        self.val = val\n    def __len__(self): \n        if not self.val:\n            return self.num_train * 2\n        return self.num_val * 2\n    def __getitem__(self, idx):\n        if self.val:\n            idx += self.num_train * 2\n        file_name = self.metadata.iloc[idx][\"Image Index\"]\n        row = self.metadata.iloc[idx].name\n        folder_num = 1\n        if row >= 4999:\n            folder_num = (row - 4999) // 10000 + 2\n        image_file_path = \"/kaggle/input/data/images_\" + str(folder_num).zfill(3) + \"/images/\" + file_name\n        img = skio.imread(image_file_path)\n        if len(img.shape) >= 3:\n            img = img[:, :, 0]\n        label = self.metadata.iloc[idx][\"Finding Labels\"]\n#         print (file_name, self.metadata.iloc[idx].name, label)\n\n        # turn 1-channel image to 3-channel image\n        img = np.stack((img,)*3, axis=-1)\n        # crop & normalize\n        img = normalize_image(img)\n        \n        if self.class_name in label:\n#             self.data[idx] = (img, 1)\n            return img, 1\n#         self.data[idx] = (img, 0)\n        return img, 0","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:23.831109Z","iopub.execute_input":"2021-12-27T08:23:23.831708Z","iopub.status.idle":"2021-12-27T08:23:23.847908Z","shell.execute_reply.started":"2021-12-27T08:23:23.831670Z","shell.execute_reply":"2021-12-27T08:23:23.847062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_train = 5000\n# _metadata = pd.read_csv(\"/kaggle/input/data/Data_Entry_2017.csv\")\n# metadata_true = _metadata.loc[_metadata[\"Finding Labels\"].str.contains(\"Effusion\")].head(num_train//2)\n# num_train = min(num_train, len(metadata_true))\n# metadata_false = _metadata.loc[~_metadata[\"Finding Labels\"].str.contains(\"Effusion\")].head(num_train//2)\n# metadata = pd.concat([metadata_true, metadata_false])\n# # len(metadata.loc[_metadata[\"Finding Labels\"].str.contains(\"Effusion\")].head(5000//2))\n\n# val = ChestDataset(class_name = \"Pneumonia\", val = True)\n# count = 0\n# for i in tqdm(range(len(val))):\n#     if val[i][1] == 1:\n#         count += 1\n# count","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:23.852408Z","iopub.execute_input":"2021-12-27T08:23:23.852804Z","iopub.status.idle":"2021-12-27T08:23:23.860899Z","shell.execute_reply.started":"2021-12-27T08:23:23.852772Z","shell.execute_reply":"2021-12-27T08:23:23.860130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_dataset = ChestDataset(class_name = \"Cardiomegaly\")\n# from random import sample\n# for i in sample(range(len(_dataset)), 10):\n#     img, label = _dataset[i]\n#     print (label)\n#     plt.imshow(img, cmap = \"gray\")\n#     plt.show()\n\n# one_channel_img = _dataset[0][0] \n# rgb_img = np.stack((one_channel_img,)*3, axis=-1)\n# show_image(rgb_img)\n# transformed_image = normalize_image(rgb_img)\n# show_image(rearrange_tensor(transformed_image))\n# show_image(unnormalize_tensor(transformed_image))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:23.862349Z","iopub.execute_input":"2021-12-27T08:23:23.863181Z","iopub.status.idle":"2021-12-27T08:23:24.171703Z","shell.execute_reply.started":"2021-12-27T08:23:23.863139Z","shell.execute_reply":"2021-12-27T08:23:24.170952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\ntrain_dataset = ChestDataset(class_name = \"Cardiomegaly\")\ntrain_data_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\nvalidation_dataset = ChestDataset(class_name =\"Cardiomegaly\", val = True)\nvalidation_data_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:24.172837Z","iopub.execute_input":"2021-12-27T08:23:24.173035Z","iopub.status.idle":"2021-12-27T08:23:24.732747Z","shell.execute_reply.started":"2021-12-27T08:23:24.173010Z","shell.execute_reply":"2021-12-27T08:23:24.732022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:24.734206Z","iopub.execute_input":"2021-12-27T08:23:24.734494Z","iopub.status.idle":"2021-12-27T08:23:24.740782Z","shell.execute_reply.started":"2021-12-27T08:23:24.734457Z","shell.execute_reply":"2021-12-27T08:23:24.739893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install vit_pytorch linformer","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:23:06.265685Z","iopub.execute_input":"2021-12-27T08:23:06.266370Z","iopub.status.idle":"2021-12-27T08:23:15.889595Z","shell.execute_reply.started":"2021-12-27T08:23:06.266331Z","shell.execute_reply":"2021-12-27T08:23:15.888466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vit_pytorch.efficient import ViT\nfrom linformer import Linformer\nefficient_transformer = Linformer(\n    dim=128,\n    seq_len=50,  # 7x7 patches + 1 cls-token\n    depth=12,\n    heads=8,\n    k=64\n)\n\nmodel = ViT(\n    dim=128,\n    image_size=224,\n    patch_size=32,\n    num_classes=2,\n    transformer=efficient_transformer,\n    channels=3,\n).to(device)\n\n# model = ViT(\n#     dim=128,\n#     image_size=224,\n#     patch_size=32,\n#     num_classes=2,\n#     transformer=efficient_transformer,\n#     channels=3,\n# ).to(device)\n\n# using pretrained resnet101 due to results found in https://www.nature.com/articles/s41598-020-74164-z\n# import torchvision.models as models\n# model = models.resnet101(pretrained=True)\n# model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n# model = model.to(device)\n# fc_layers = nn.Linear(in_features=1000, out_features=2,bias=True)\n# fc_layers = fc_layers.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:29:58.739001Z","iopub.execute_input":"2021-12-27T08:29:58.739755Z","iopub.status.idle":"2021-12-27T08:29:58.788268Z","shell.execute_reply.started":"2021-12-27T08:29:58.739693Z","shell.execute_reply":"2021-12-27T08:29:58.787626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epochs = 25\n# lr = 3e-5\n# gamma = 0.7\n\nepochs = 6\nlr = 3e-4\ngamma = 0.1","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:29:59.557953Z","iopub.execute_input":"2021-12-27T08:29:59.558408Z","iopub.status.idle":"2021-12-27T08:29:59.562385Z","shell.execute_reply.started":"2021-12-27T08:29:59.558371Z","shell.execute_reply":"2021-12-27T08:29:59.561446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss function\n# triplet and siamese losses are supposed to be used for typical CBIR tasks\ncriterion = nn.CrossEntropyLoss() # nn.TripletMarginLoss(margin=0.1) \noptimizer = optim.Adam(model.parameters(), lr=lr)\nscheduler = StepLR(optimizer, step_size=1, gamma=gamma)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:30:00.785666Z","iopub.execute_input":"2021-12-27T08:30:00.786253Z","iopub.status.idle":"2021-12-27T08:30:00.792527Z","shell.execute_reply.started":"2021-12-27T08:30:00.786211Z","shell.execute_reply":"2021-12-27T08:30:00.791338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\ncheckpoint_file = \"model_weights\"\nepoch = 0\n# if os.path.exists(checkpoint_file):\n#     print('gathering info from checkpoint file')\n#     checkpoint = torch.load(checkpoint_file)\n#     model.load_state_dict(checkpoint['model_state_dict'])\n#     criterion.load_state_dict(checkpoint['optimizer_state_dict'])\n#     epoch = checkpoint['epoch'] + 1\n\nfor epoch in range(epoch, epochs + epoch):\n    epoch_loss = 0\n    epoch_accuracy = 0\n    \n    # Wrap in a progress bar to display progress during training.\n    progress_bar = tqdm.tqdm(train_data_loader)\n    \n    for i, inputs in enumerate(progress_bar):\n        data, label = inputs\n        data = data.to(device)\n        label = label.to(device)\n        optimizer.zero_grad()\n\n        output = model(data)\n        \n        loss = criterion(output, label)\n        \n        loss.backward()\n        optimizer.step()\n        progress_bar.set_description(f\"Loss: {loss}\")\n        \n        acc = (output.argmax(dim=1) == label).float().mean()\n        epoch_accuracy += acc / len(train_data_loader)\n        epoch_loss += loss / len(train_data_loader)\n    with torch.no_grad():\n        epoch_val_accuracy = 0\n        epoch_val_loss = 0\n        for data, label in tqdm.tqdm(validation_data_loader):\n            data = data.to(device)\n            label = label.to(device)\n\n            val_output = model(data)\n            val_loss = criterion(val_output, label)\n            \n\n            acc = (val_output.argmax(dim=1) == label).float().mean()\n            epoch_val_accuracy += acc / len(validation_data_loader)\n            epoch_val_loss += val_loss / len(validation_data_loader)\n    torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': criterion.state_dict(),\n                'acc': epoch_accuracy,\n                'loss': epoch_loss,\n                'validation_acc' : epoch_val_accuracy,\n                'validation_loss': epoch_val_loss,\n                }, checkpoint_file)\n    print(\n        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:30:01.096590Z","iopub.execute_input":"2021-12-27T08:30:01.096967Z","iopub.status.idle":"2021-12-27T08:46:07.825148Z","shell.execute_reply.started":"2021-12-27T08:30:01.096935Z","shell.execute_reply":"2021-12-27T08:46:07.824417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('output argmax:', output.argmax(dim=1))\nprint(len(output))\nprint(len(label))\nprint(label.shape, output.shape)\ncriterion(output, label)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.556494Z","iopub.status.idle":"2021-12-27T07:50:47.55692Z","shell.execute_reply.started":"2021-12-27T07:50:47.556687Z","shell.execute_reply":"2021-12-27T07:50:47.55671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.558076Z","iopub.status.idle":"2021-12-27T07:50:47.560061Z","shell.execute_reply.started":"2021-12-27T07:50:47.559813Z","shell.execute_reply":"2021-12-27T07:50:47.559848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_output = model(data)\n# val_loss = criterion(val_output, label)\n\n# acc = (val_output.argmax(dim=1) == label).float().mean()\n# print('val_output:', val_output)\nprint('val_output argmax:', val_output.argmax(dim=1))\nprint(len(val_output))\nprint(len(label))\nprint(label.shape, val_output.shape)\nprint (data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.561153Z","iopub.status.idle":"2021-12-27T07:50:47.561761Z","shell.execute_reply.started":"2021-12-27T07:50:47.561509Z","shell.execute_reply":"2021-12-27T07:50:47.561534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/model-weights/model_weights\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\ncriterion.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch'] + 1","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.563292Z","iopub.status.idle":"2021-12-27T07:50:47.563719Z","shell.execute_reply.started":"2021-12-27T07:50:47.563494Z","shell.execute_reply":"2021-12-27T07:50:47.563517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'model_weights')","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.565149Z","iopub.status.idle":"2021-12-27T07:50:47.565569Z","shell.execute_reply.started":"2021-12-27T07:50:47.565345Z","shell.execute_reply":"2021-12-27T07:50:47.565368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from einops import rearrange, repeat\ndef get_latent(vit_model, img):\n    x = vit_model.to_patch_embedding(img)\n    b, n, _ = x.shape\n\n    cls_tokens = repeat(vit_model.cls_token, '() n d -> b n d', b = b)\n    x = torch.cat((cls_tokens, x), dim=1)\n    x += vit_model.pos_embedding[:, :(n + 1)]\n    x = vit_model.transformer(x)\n\n    x = x.mean(dim = 1) if vit_model.pool == 'mean' else x[:, 0]\n\n    return vit_model.to_latent(x)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:47:31.530429Z","iopub.execute_input":"2021-12-27T08:47:31.531087Z","iopub.status.idle":"2021-12-27T08:47:31.536693Z","shell.execute_reply.started":"2021-12-27T08:47:31.531047Z","shell.execute_reply":"2021-12-27T08:47:31.536041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unshuffled_train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:47:59.262070Z","iopub.execute_input":"2021-12-27T08:47:59.262839Z","iopub.status.idle":"2021-12-27T08:47:59.267303Z","shell.execute_reply.started":"2021-12-27T08:47:59.262718Z","shell.execute_reply":"2021-12-27T08:47:59.266584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = np.zeros((len(train_dataset), 128))\nlabels = []\nmodel.eval()\nimgs = []\nfor i, sample in enumerate(tqdm.tqdm(unshuffled_train_dataloader)):\n    img, label = sample\n    labels.append(int(label))\n    img = img.to(device)\n    label = label.to(device)\n    embedding = get_latent(model, img)\n    embeddings[i] = (embedding.cpu().detach().numpy())\n    original_image = unnormalize_img(einops.rearrange(img.cpu().detach().numpy(), \"b c w h -> w h (b c)\"))\n    imgs.append(original_image)\nembeddings = np.array(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:49:06.189085Z","iopub.execute_input":"2021-12-27T08:49:06.189958Z","iopub.status.idle":"2021-12-27T08:51:13.002172Z","shell.execute_reply.started":"2021-12-27T08:49:06.189903Z","shell.execute_reply":"2021-12-27T08:51:13.000794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnp.save(\"imgs.npy\", imgs)\n\nfrom IPython.display import FileLink\nFileLink(r'imgs.npy')","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:09:38.769931Z","iopub.execute_input":"2021-12-27T07:09:38.77019Z","iopub.status.idle":"2021-12-27T07:09:43.791006Z","shell.execute_reply.started":"2021-12-27T07:09:38.77016Z","shell.execute_reply":"2021-12-27T07:09:43.789978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n# out some marginal improvements. NB: This takes almost an hour!\ntsne = TSNE()\n\nlow_dim_embeddings = tsne.fit_transform(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:51:13.003793Z","iopub.execute_input":"2021-12-27T08:51:13.004494Z","iopub.status.idle":"2021-12-27T08:51:44.402539Z","shell.execute_reply.started":"2021-12-27T08:51:13.004453Z","shell.execute_reply":"2021-12-27T08:51:44.401767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmap = {0: 'red', 1: 'blue'}\nc = labels\nplt.scatter(low_dim_embeddings[:, 0], low_dim_embeddings[:, 1], c=c, cmap = \"Accent\")\nplt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:51:44.404183Z","iopub.execute_input":"2021-12-27T08:51:44.404464Z","iopub.status.idle":"2021-12-27T08:51:44.795217Z","shell.execute_reply.started":"2021-12-27T08:51:44.404424Z","shell.execute_reply":"2021-12-27T08:51:44.794543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import random\nfrom scipy.spatial import distance\n\ndef closest_node(node, nodes):\n    return np.argsort(distance.cdist(node, nodes))[:, :5][0]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:51:44.797210Z","iopub.execute_input":"2021-12-27T08:51:44.797689Z","iopub.status.idle":"2021-12-27T08:51:44.802595Z","shell.execute_reply.started":"2021-12-27T08:51:44.797650Z","shell.execute_reply":"2021-12-27T08:51:44.801960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\n\ndef image_grid(img_arr):\n    fig = plt.figure(figsize=(20., 20.))\n    grid = ImageGrid(fig, 111, \n                     nrows_ncols=(1, 5),  # creates 2x2 grid of axes\n                     axes_pad=0.1,  # pad between axes\n                     )\n\n    for ax, im in zip(grid, img_arr):\n        ax.imshow(im)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:51:44.804143Z","iopub.execute_input":"2021-12-27T08:51:44.804615Z","iopub.status.idle":"2021-12-27T08:51:44.814116Z","shell.execute_reply.started":"2021-12-27T08:51:44.804579Z","shell.execute_reply":"2021-12-27T08:51:44.813258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = closest_node(np.array([[40,40]]), low_dim_embeddings)\ntop_images = np.array(imgs)[indices]\nprint(\"CARDIOMEGALY IMAGES:\")\nprint(np.array(labels)[indices])\nimage_grid(top_images)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:52:17.750842Z","iopub.execute_input":"2021-12-27T08:52:17.751126Z","iopub.status.idle":"2021-12-27T08:52:20.864139Z","shell.execute_reply.started":"2021-12-27T08:52:17.751094Z","shell.execute_reply":"2021-12-27T08:52:20.862687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = closest_node(np.array([[20,-100]]), low_dim_embeddings)\ntop_images = np.array(imgs)[indices]\nprint(\"NO FINDING IMAGES:\")\nprint(np.array(labels)[indices])\nimage_grid(top_images)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:53:28.186423Z","iopub.execute_input":"2021-12-27T08:53:28.187195Z","iopub.status.idle":"2021-12-27T08:53:30.871132Z","shell.execute_reply.started":"2021-12-27T08:53:28.187140Z","shell.execute_reply":"2021-12-27T08:53:30.869527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax  = fig.add_subplot(111)\ncmap = {0: 'red', 1: 'blue'}\nc = labels\nx = low_dim_embeddings[:, 0]\ny = low_dim_embeddings[:, 1]\nax.scatter(low_dim_embeddings[:, 0], low_dim_embeddings[:, 1], c=c, cmap = \"Accent\")\n# ax.colorbar()\n\ndef onclick(event):\n    ix, iy = event.xdata, event.ydata\n    print(\"I clicked at x={0:5.2f}, y={1:5.2f}\".format(ix,iy))\n\n    # Calculate, based on the axis extent, a reasonable distance \n    # from the actual point in which the click has to occur (in this case 5%)\n    ax = plt.gca()\n    dx = 0.05 * (ax.get_xlim()[1] - ax.get_xlim()[0])\n    dy = 0.05 * (ax.get_ylim()[1] - ax.get_ylim()[0])\n\n    # Check for every point if the click was close enough:\n    for i in range(len(x)):\n        if(x[i] > ix-dx and x[i] < ix+dx and y[i] > iy-dy and y[i] < iy+dy):\n            plt.imshow(imgs[i])\n            print(\"You clicked close enough!\")\n\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}