{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import  nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm.notebook import tqdm\nimport cv2\nfrom torchvision import transforms, utils\nimport skimage.io as skio\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-27T07:39:28.646194Z","iopub.execute_input":"2021-12-27T07:39:28.646419Z","iopub.status.idle":"2021-12-27T07:39:32.262005Z","shell.execute_reply.started":"2021-12-27T07:39:28.646392Z","shell.execute_reply":"2021-12-27T07:39:32.261267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_metadata = pd.read_csv(\"/kaggle/input/data/Data_Entry_2017.csv\")\n_metadata.loc[_metadata[\"Finding Labels\"].str.contains(\"No Finding\")]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:32.263702Z","iopub.execute_input":"2021-12-27T07:39:32.263970Z","iopub.status.idle":"2021-12-27T07:39:33.054211Z","shell.execute_reply.started":"2021-12-27T07:39:32.263934Z","shell.execute_reply":"2021-12-27T07:39:33.053550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:33.055569Z","iopub.execute_input":"2021-12-27T07:39:33.055862Z","iopub.status.idle":"2021-12-27T07:39:42.133878Z","shell.execute_reply.started":"2021-12-27T07:39:33.055823Z","shell.execute_reply":"2021-12-27T07:39:42.133047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import einops\ndef show_image(image):\n    plt.figure()\n    plt.imshow(image)\n    plt.show()\n\ndef rearrange_tensor(img):\n    return einops.rearrange(img, \"c w h -> w h c\")\n\ndef unnormalize_tensor(img):   \n    img = img.detach().numpy()\n#     img = einops.rearrange(img, \"b c w h -> w h (b c)\")\n    img = einops.rearrange(img, \"c w h -> w h c\")\n\n    mean = np.array([0.485, 0.456, 0.406])\n    std =  np.array([0.229, 0.224, 0.225])\n    img = (img * std) + mean\n    return img\n\ndef unnormalize_img(img):   \n    mean = np.array([0.485, 0.456, 0.406])\n    std =  np.array([0.229, 0.224, 0.225])\n    img = (img * std) + mean\n    return img\n\n\ndef normalize_image(img):\n    # normalize all images, this is necessary prepreocessing of inputs for vgg network\n    normalize = transforms.Compose([\n                    transforms.ToPILImage(),\n                    transforms.Resize(256), \n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.485, 0.456, 0.406), \n                                          (0.229, 0.224, 0.225))\n                    ])\n    img = normalize(img)\n#     img = img[:3,:,:].unsqueeze(0)\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:59:14.203245Z","iopub.execute_input":"2021-12-27T07:59:14.203505Z","iopub.status.idle":"2021-12-27T07:59:14.212660Z","shell.execute_reply.started":"2021-12-27T07:59:14.203475Z","shell.execute_reply":"2021-12-27T07:59:14.211756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChestDataset(Dataset):\n    def __init__(self, class_name = \"Pneumonia\", val = False):\n        _metadata = pd.read_csv(\"/kaggle/input/data/Data_Entry_2017.csv\")\n        _metadata = _metadata.sample(frac = 1)\n        metadata_true = _metadata.loc[_metadata[\"Finding Labels\"].str.contains(class_name)]\n        metadata_false = _metadata.loc[_metadata[\"Finding Labels\"].str.contains(\"No Finding\")]\n        num_train = int(.6 * len(metadata_true))\n        num_val = int(.4 * len(metadata_true))\n        train_metadata = pd.concat([metadata_true[:num_train], metadata_false[:num_train]])\n        val_metadata = pd.concat([metadata_true[num_train:], metadata_false[num_train : num_train + num_val]])\n        self.metadata = pd.concat([train_metadata, val_metadata])\n        self.num_train = num_train\n        self.num_val = num_val\n        self.class_name = class_name\n        self.data = {}\n        self.val = val\n    def __len__(self): \n        if not self.val:\n            return self.num_train * 2\n        return self.num_val * 2\n    def __getitem__(self, idx):\n        if self.val:\n            idx += self.num_train * 2\n        file_name = self.metadata.iloc[idx][\"Image Index\"]\n        row = self.metadata.iloc[idx].name\n        folder_num = 1\n        if row >= 4999:\n            folder_num = (row - 4999) // 10000 + 2\n        image_file_path = \"/kaggle/input/data/images_\" + str(folder_num).zfill(3) + \"/images/\" + file_name\n        img = skio.imread(image_file_path)\n        if len(img.shape) >= 3:\n            img = img[:, :, 0]\n        label = self.metadata.iloc[idx][\"Finding Labels\"]\n#         print (file_name, self.metadata.iloc[idx].name, label)\n\n        # turn 1-channel image to 3-channel image\n        img = np.stack((img,)*3, axis=-1)\n        # crop & normalize\n        img = normalize_image(img)\n        \n        if self.class_name in label:\n#             self.data[idx] = (img, 1)\n            return img, 1\n#         self.data[idx] = (img, 0)\n        return img, 0","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:42.154106Z","iopub.execute_input":"2021-12-27T07:39:42.154398Z","iopub.status.idle":"2021-12-27T07:39:42.169035Z","shell.execute_reply.started":"2021-12-27T07:39:42.154357Z","shell.execute_reply":"2021-12-27T07:39:42.168074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# num_train = 5000\n# _metadata = pd.read_csv(\"/kaggle/input/data/Data_Entry_2017.csv\")\n# metadata_true = _metadata.loc[_metadata[\"Finding Labels\"].str.contains(\"Effusion\")].head(num_train//2)\n# num_train = min(num_train, len(metadata_true))\n# metadata_false = _metadata.loc[~_metadata[\"Finding Labels\"].str.contains(\"Effusion\")].head(num_train//2)\n# metadata = pd.concat([metadata_true, metadata_false])\n# # len(metadata.loc[_metadata[\"Finding Labels\"].str.contains(\"Effusion\")].head(5000//2))\n\n# val = ChestDataset(class_name = \"Pneumonia\", val = True)\n# count = 0\n# for i in tqdm(range(len(val))):\n#     if val[i][1] == 1:\n#         count += 1\n# count","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:42.170420Z","iopub.execute_input":"2021-12-27T07:39:42.170855Z","iopub.status.idle":"2021-12-27T07:39:42.181019Z","shell.execute_reply.started":"2021-12-27T07:39:42.170803Z","shell.execute_reply":"2021-12-27T07:39:42.180066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_dataset = ChestDataset(class_name = \"Cardiomegaly\")\n# from random import sample\n# for i in sample(range(len(_dataset)), 10):\n#     img, label = _dataset[i]\n#     print (label)\n#     plt.imshow(img, cmap = \"gray\")\n#     plt.show()\n\n# one_channel_img = _dataset[0][0] \n# rgb_img = np.stack((one_channel_img,)*3, axis=-1)\n# show_image(rgb_img)\n# transformed_image = normalize_image(rgb_img)\n# show_image(rearrange_tensor(transformed_image))\n# show_image(unnormalize_tensor(transformed_image))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:42.182648Z","iopub.execute_input":"2021-12-27T07:39:42.183244Z","iopub.status.idle":"2021-12-27T07:39:42.508220Z","shell.execute_reply.started":"2021-12-27T07:39:42.183203Z","shell.execute_reply":"2021-12-27T07:39:42.507494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\ntrain_dataset = ChestDataset(class_name = \"Cardiomegaly\")\ntrain_data_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\nvalidation_dataset = ChestDataset(class_name =\"Cardiomegaly\", val = True)\nvalidation_data_loader = DataLoader(validation_dataset, batch_size = batch_size, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:42.509596Z","iopub.execute_input":"2021-12-27T07:39:42.509887Z","iopub.status.idle":"2021-12-27T07:39:43.105398Z","shell.execute_reply.started":"2021-12-27T07:39:42.509850Z","shell.execute_reply":"2021-12-27T07:39:43.104646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:43.107745Z","iopub.execute_input":"2021-12-27T07:39:43.108239Z","iopub.status.idle":"2021-12-27T07:39:43.161012Z","shell.execute_reply.started":"2021-12-27T07:39:43.108198Z","shell.execute_reply":"2021-12-27T07:39:43.160195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip -q install vit_pytorch linformer","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:43.164592Z","iopub.execute_input":"2021-12-27T07:39:43.165050Z","iopub.status.idle":"2021-12-27T07:39:43.170022Z","shell.execute_reply.started":"2021-12-27T07:39:43.165010Z","shell.execute_reply":"2021-12-27T07:39:43.168895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from vit_pytorch.efficient import ViT\n# from linformer import Linformer\n# efficient_transformer = Linformer(\n#     dim=128,\n#     seq_len=1025,  # 7x7 patches + 1 cls-token\n#     depth=12,\n#     heads=8,\n#     k=64\n# )\n\n# model = ViT(\n#     dim=128,\n#     image_size=1024,\n#     patch_size=16,\n#     num_classes=2,\n#     transformer=efficient_transformer,\n#     channels=1,\n# ).to(device)\n\n# using pretrained resnet101 due to results found in https://www.nature.com/articles/s41598-020-74164-z\nimport torchvision.models as models\nmodel = models.resnet101(pretrained=True)\n# model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nmodel = model.to(device)\nfc_layers = nn.Linear(in_features=1000, out_features=2,bias=True)\nfc_layers = fc_layers.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:43.171940Z","iopub.execute_input":"2021-12-27T07:39:43.172313Z","iopub.status.idle":"2021-12-27T07:39:54.304713Z","shell.execute_reply.started":"2021-12-27T07:39:43.172274Z","shell.execute_reply":"2021-12-27T07:39:54.303908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epochs = 25\n# lr = 3e-5\n# gamma = 0.7\n\nepochs = 6\nlr = 3e-4\ngamma = 0.1","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:54.306209Z","iopub.execute_input":"2021-12-27T07:39:54.306472Z","iopub.status.idle":"2021-12-27T07:39:54.310781Z","shell.execute_reply.started":"2021-12-27T07:39:54.306435Z","shell.execute_reply":"2021-12-27T07:39:54.309773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss function\n# triplet and siamese losses are supposed to be used for typical CBIR tasks\ncriterion = nn.CrossEntropyLoss() # nn.TripletMarginLoss(margin=0.1) \n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n# scheduler\nscheduler = StepLR(optimizer, step_size=1, gamma=gamma)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:54.312328Z","iopub.execute_input":"2021-12-27T07:39:54.312907Z","iopub.status.idle":"2021-12-27T07:39:54.324421Z","shell.execute_reply.started":"2021-12-27T07:39:54.312866Z","shell.execute_reply":"2021-12-27T07:39:54.323689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\ncheckpoint_file = \"model_weights\"\nepoch = 0\n# if os.path.exists(checkpoint_file):\n#     print('gathering info from checkpoint file')\n#     checkpoint = torch.load(checkpoint_file)\n#     model.load_state_dict(checkpoint['model_state_dict'])\n#     criterion.load_state_dict(checkpoint['optimizer_state_dict'])\n#     epoch = checkpoint['epoch'] + 1\n\nfor epoch in range(epoch, epochs + epoch):\n    epoch_loss = 0\n    epoch_accuracy = 0\n    \n    # Wrap in a progress bar to display progress during training.\n    progress_bar = tqdm.tqdm(train_data_loader)\n    \n    for i, inputs in enumerate(progress_bar):\n        data, label = inputs\n        data = data.to(device)\n        label = label.to(device)\n        optimizer.zero_grad()\n\n        output = fc_layers(model(data))\n        \n        loss = criterion(output, label)\n        \n        loss.backward()\n        optimizer.step()\n        progress_bar.set_description(f\"Loss: {loss}\")\n        \n        acc = (output.argmax(dim=1) == label).float().mean()\n        epoch_accuracy += acc / len(train_data_loader)\n        epoch_loss += loss / len(train_data_loader)\n    with torch.no_grad():\n        epoch_val_accuracy = 0\n        epoch_val_loss = 0\n        for data, label in tqdm.tqdm(validation_data_loader):\n            data = data.to(device)\n            label = label.to(device)\n\n            val_output = fc_layers(model(data))\n            val_loss = criterion(val_output, label)\n            \n\n            acc = (val_output.argmax(dim=1) == label).float().mean()\n            epoch_val_accuracy += acc / len(validation_data_loader)\n            epoch_val_loss += val_loss / len(validation_data_loader)\n    torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': criterion.state_dict(),\n                'acc': epoch_accuracy,\n                'loss': epoch_loss,\n                'validation_acc' : epoch_val_accuracy,\n                'validation_loss': epoch_val_loss,\n                }, checkpoint_file)\n    print(\n        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n    )","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:39:54.325944Z","iopub.execute_input":"2021-12-27T07:39:54.326632Z","iopub.status.idle":"2021-12-27T07:50:47.555191Z","shell.execute_reply.started":"2021-12-27T07:39:54.326589Z","shell.execute_reply":"2021-12-27T07:50:47.553988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('output argmax:', output.argmax(dim=1))\nprint(len(output))\nprint(len(label))\nprint(label.shape, output.shape)\ncriterion(output, label)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.556494Z","iopub.status.idle":"2021-12-27T07:50:47.556920Z","shell.execute_reply.started":"2021-12-27T07:50:47.556687Z","shell.execute_reply":"2021-12-27T07:50:47.556710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.558076Z","iopub.status.idle":"2021-12-27T07:50:47.560061Z","shell.execute_reply.started":"2021-12-27T07:50:47.559813Z","shell.execute_reply":"2021-12-27T07:50:47.559848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_output = model(data)\n# val_loss = criterion(val_output, label)\n\n# acc = (val_output.argmax(dim=1) == label).float().mean()\n# print('val_output:', val_output)\nprint('val_output argmax:', val_output.argmax(dim=1))\nprint(len(val_output))\nprint(len(label))\nprint(label.shape, val_output.shape)\nprint (data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.561153Z","iopub.status.idle":"2021-12-27T07:50:47.561761Z","shell.execute_reply.started":"2021-12-27T07:50:47.561509Z","shell.execute_reply":"2021-12-27T07:50:47.561534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/model-weights/model_weights\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\ncriterion.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch'] + 1","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.563292Z","iopub.status.idle":"2021-12-27T07:50:47.563719Z","shell.execute_reply.started":"2021-12-27T07:50:47.563494Z","shell.execute_reply":"2021-12-27T07:50:47.563517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'model_weights')","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:50:47.565149Z","iopub.status.idle":"2021-12-27T07:50:47.565569Z","shell.execute_reply.started":"2021-12-27T07:50:47.565345Z","shell.execute_reply":"2021-12-27T07:50:47.565368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from einops import rearrange, repeat\ndef get_latent(vit_model, img):\n    x = vit_model.to_patch_embedding(img)\n    b, n, _ = x.shape\n\n    cls_tokens = repeat(vit_model.cls_token, '() n d -> b n d', b = b)\n    x = torch.cat((cls_tokens, x), dim=1)\n    x += vit_model.pos_embedding[:, :(n + 1)]\n    x = vit_model.transformer(x)\n\n    x = x.mean(dim = 1) if vit_model.pool == 'mean' else x[:, 0]\n\n    return vit_model.to_latent(x)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unshuffled_train_dataloader = DataLoader(train_dataset, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:51:04.066402Z","iopub.execute_input":"2021-12-27T07:51:04.067033Z","iopub.status.idle":"2021-12-27T07:51:04.071029Z","shell.execute_reply.started":"2021-12-27T07:51:04.066988Z","shell.execute_reply":"2021-12-27T07:51:04.070086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = np.zeros((len(train_dataset), 1000))\nlabels = []\nmodel.eval()\nimgs = []\nfor i, sample in enumerate(tqdm.tqdm(unshuffled_train_dataloader)):\n#     if i >= 400:\n#         break\n    img, label = sample\n#     print (label)\n    labels.append(int(label))\n    img = img.to(device)\n    label = label.to(device)\n#     img = img.unsqueeze(1)\n\n#     embedding = get_latent(model, img)\n    embedding = model(img)\n    embeddings[i] = (embedding.cpu().detach().numpy())\n    original_image = unnormalize_img(einops.rearrange(img.cpu().detach().numpy(), \"b c w h -> w h (b c)\"))\n    imgs.append(original_image)\nembeddings = np.array(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:00:30.561422Z","iopub.execute_input":"2021-12-27T08:00:30.562303Z","iopub.status.idle":"2021-12-27T08:03:07.929587Z","shell.execute_reply.started":"2021-12-27T08:00:30.562261Z","shell.execute_reply":"2021-12-27T08:03:07.928756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnp.save(\"imgs.npy\", imgs)\n\nfrom IPython.display import FileLink\nFileLink(r'imgs.npy')","metadata":{"execution":{"iopub.status.busy":"2021-12-27T07:09:38.769931Z","iopub.execute_input":"2021-12-27T07:09:38.770190Z","iopub.status.idle":"2021-12-27T07:09:43.791006Z","shell.execute_reply.started":"2021-12-27T07:09:38.770160Z","shell.execute_reply":"2021-12-27T07:09:43.789978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n# out some marginal improvements. NB: This takes almost an hour!\ntsne = TSNE()\n\nlow_dim_embeddings = tsne.fit_transform(embeddings)\n# low_dim_embeddings = embeddings","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:03:07.931337Z","iopub.execute_input":"2021-12-27T08:03:07.931760Z","iopub.status.idle":"2021-12-27T08:03:38.063574Z","shell.execute_reply.started":"2021-12-27T08:03:07.931706Z","shell.execute_reply":"2021-12-27T08:03:38.062855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmap = {0: 'red', 1: 'blue'}\nc = labels\nplt.scatter(low_dim_embeddings[:, 0], low_dim_embeddings[:, 1], c=c, cmap = \"Accent\")\nplt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:03:38.064980Z","iopub.execute_input":"2021-12-27T08:03:38.065273Z","iopub.status.idle":"2021-12-27T08:03:38.441243Z","shell.execute_reply.started":"2021-12-27T08:03:38.065234Z","shell.execute_reply":"2021-12-27T08:03:38.440584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import random\nfrom scipy.spatial import distance\n\ndef closest_node(node, nodes):\n#     print (distance.cdist(node, nodes).shape)\n    return np.argsort(distance.cdist(node, nodes))[:, :5][0]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:05:25.859344Z","iopub.execute_input":"2021-12-27T08:05:25.859595Z","iopub.status.idle":"2021-12-27T08:05:25.866807Z","shell.execute_reply.started":"2021-12-27T08:05:25.859567Z","shell.execute_reply":"2021-12-27T08:05:25.866107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\n\ndef image_grid(img_arr):\n    fig = plt.figure(figsize=(20., 20.))\n    grid = ImageGrid(fig, 111, \n                     nrows_ncols=(1, 5),  # creates 2x2 grid of axes\n                     axes_pad=0.1,  # pad between axes\n                     )\n\n    for ax, im in zip(grid, img_arr):\n        ax.imshow(im)\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:13:48.760113Z","iopub.execute_input":"2021-12-27T08:13:48.760375Z","iopub.status.idle":"2021-12-27T08:13:48.766632Z","shell.execute_reply.started":"2021-12-27T08:13:48.760343Z","shell.execute_reply":"2021-12-27T08:13:48.765963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = closest_node(np.array([[40,40]]), low_dim_embeddings)\ntop_images = np.array(imgs)[indices]\nprint(\"NO FINDING IMAGES:\")\nprint(np.array(labels)[indices])\nimage_grid(top_images)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:14:56.300765Z","iopub.execute_input":"2021-12-27T08:14:56.301239Z","iopub.status.idle":"2021-12-27T08:14:59.135120Z","shell.execute_reply.started":"2021-12-27T08:14:56.301202Z","shell.execute_reply":"2021-12-27T08:14:59.134041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indices = closest_node(np.array([[60,-60]]), low_dim_embeddings)\ntop_images = np.array(imgs)[indices]\nprint(\"CARDIOMEGALY IMAGES:\")\nprint(np.array(labels)[indices])\nimage_grid(top_images)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T08:17:30.974023Z","iopub.execute_input":"2021-12-27T08:17:30.974482Z","iopub.status.idle":"2021-12-27T08:17:33.747216Z","shell.execute_reply.started":"2021-12-27T08:17:30.974443Z","shell.execute_reply":"2021-12-27T08:17:33.746568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.image as mpimg\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax  = fig.add_subplot(111)\ncmap = {0: 'red', 1: 'blue'}\nc = labels\nx = low_dim_embeddings[:, 0]\ny = low_dim_embeddings[:, 1]\nax.scatter(low_dim_embeddings[:, 0], low_dim_embeddings[:, 1], c=c, cmap = \"Accent\")\n# ax.colorbar()\n\ndef onclick(event):\n    ix, iy = event.xdata, event.ydata\n    print(\"I clicked at x={0:5.2f}, y={1:5.2f}\".format(ix,iy))\n\n    # Calculate, based on the axis extent, a reasonable distance \n    # from the actual point in which the click has to occur (in this case 5%)\n    ax = plt.gca()\n    dx = 0.05 * (ax.get_xlim()[1] - ax.get_xlim()[0])\n    dy = 0.05 * (ax.get_ylim()[1] - ax.get_ylim()[0])\n\n    # Check for every point if the click was close enough:\n    for i in range(len(x)):\n        if(x[i] > ix-dx and x[i] < ix+dx and y[i] > iy-dy and y[i] < iy+dy):\n            plt.imshow(imgs[i])\n            print(\"You clicked close enough!\")\n\ncid = fig.canvas.mpl_connect('button_press_event', onclick)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}